{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38681f-f756-494e-abed-294622386bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download\n",
    "## curl?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c8d6757b-c202-496f-b8b7-03e36094c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import PartialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6213d191-d48f-470c-86a6-cc8cb233d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_state = PartialState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "127b1412-1bae-4142-8ae7-0fbc68efb57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DistributedType.NO: 'NO'>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_state.distributed_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a2adea5d-989d-4ea1-af49-7b1bade31ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a19c02cd-51c7-458a-b9fb-f58ff1a0c539",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[144], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "ds['train'].to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "270c4e48-eab5-4b75-bb71-59e87b6393d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_state.is_main_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "df4ea1ea-9649-4adf-8461-dcfce79bc037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "071ab612-af8d-4ae3-afa6-fc17e0325dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8310d8bd-ddb5-45a0-b4df-1bf8f8ee294a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d5634cd3-35c7-4208-b0fb-72329284389c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b9489f38-27b2-4d67-9577-41c7d6cd9cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "310f85e5-886b-45c3-8950-7ff309b46cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 392702/392702 [00:33<00:00, 11865.45it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(ds['train']):\n",
    "    i['test']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3036a3b7-9393-4e4a-9ec7-6bf63b345c03",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Dataset.to_json() missing 1 required positional argument: 'path_or_buf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Dataset.to_json() missing 1 required positional argument: 'path_or_buf'"
     ]
    }
   ],
   "source": [
    "ds['train'].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "82ff67d1-f5c9-4562-856f-eb209b353ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "    num_rows: 392702\n",
       "})"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7a60d-ace8-4e44-917b-3458517f16bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882016f-d035-4d95-90c5-f6ef1a7e3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "927d302b-0eba-44f5-86ac-3e6550cdec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a5321350-2cf9-410d-ba81-e98b4344e78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     1,\n",
       "          6097,   895, 29901,  1281,  1547,  1474,   907,   314,  2071, 25217,\n",
       "           756,  1023,  6996, 13391,   448,  3234,   322,  1737,  5275, 29889,\n",
       "            13,    13, 29950,  1478,   720,  6656, 29901, 10969,   322,  1737,\n",
       "          5275,   526,   825,  1207,   907,   314,  2071, 25217,   664, 29889,\n",
       "         29871],\n",
       "        [    1,  6097,   895, 29901,   366,  1073,  2645,   278,  4259,   322,\n",
       "           474,  4140,   472,   472,   596,  3233,   318, 29882,   366, 14074,\n",
       "           963,   304,   278,  2446,  3233,   565,   565,   896, 11097,   304,\n",
       "         17386,   278,   278,  3847,  3815,   278,  5032,  1960, 11097,   304,\n",
       "          1246,   304, 17386,   263,  1410, 29891,   515, 21954,   319,   769,\n",
       "           263,  3765,   319,  1410, 29891,  5771,   701,   304,  5191,  1075,\n",
       "           322,   263,  2323,   319,  1410, 29891,  5771,   701,   304,  5191,\n",
       "          1075,    13,    13, 29950,  1478,   720,  6656, 29901,   887, 14074,\n",
       "           278,  2712,   304,   278,  1494,  3233,   565,   278,  2305, 17386,\n",
       "         29889],\n",
       "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     1,  6097,   895, 29901,  3118,\n",
       "           310,  1749,  1353,   674,  8677,   714,   596, 11994,  1375, 11579,\n",
       "         29889,    13,    13, 29950,  1478,   720,  6656, 29901,   319,  4509,\n",
       "           310,   590,  3815,   674,  6222,   596, 11299,   411, 29403, 16716,\n",
       "         29889],\n",
       "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     1,  6097,   895, 29901,  1128,   437,   366,  1073,\n",
       "         29973,  2178,   445,   338,  1009,  2472,  1449, 29889,    13,    13,\n",
       "         29950,  1478,   720,  6656, 29901,   910,  2472, 14393,   304,   963,\n",
       "         29889],\n",
       "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     1,  6097,   895, 29901,\n",
       "         21915,   474,  2649,   366,   825,  2466,   565,   366,   748,  8666,\n",
       "           777,   310,  1906, 22556, 17394,   267,   474,   508,  1074,  2020,\n",
       "          1286,   366,  1073,   896, 29915,   276,  2805,   701,   297,   278,\n",
       "          6893, 11232,   279,  3464,    13,    13, 29950,  1478,   720,  6656,\n",
       "         29901,   450, 22556, 17394,   267,   505,   263,  3464,   310, 26094,\n",
       "         29889],\n",
       "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     1,  6097,   895, 29901,   590,  6686,  1171,\n",
       "         14455,   577,   474, 29915, 29885, 24081,   300,  1286,   474,   925,\n",
       "           505,   304,  2507,   278,   269, 12358, 29877,   701,  1855, 22526,\n",
       "            13,    13, 29950,  1478,   720,  6656, 29901,   306, 29915, 29885,\n",
       "         24081,   300,   393,   590,  6686,  1171, 14455,   322,  1286,   306,\n",
       "           505,   304,  2507,   278,   269, 12358, 29877,   701,  2289, 22526,\n",
       "         29889],\n",
       "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     1,  6097,   895, 29901,  1205,\n",
       "           263,  2846,  6111,   286,  3628,  1199, 10503,   573,  2038,   278,\n",
       "          3095,   344,   338,   278,  9167,   411,   278, 28056, 13825, 29892,\n",
       "           411,   278,  2595,  9477, 18672,   304,   278,  1492,   313, 22880,\n",
       "         18708,  5765, 29892,   304,   278,  2175, 29892,   756,  1109,  3276,\n",
       "          4078,   363,   263,  2846,  1238, 19467,   515,   670, 24745,   467,\n",
       "            13,    13, 29950,  1478,   720,  6656, 29901,  7849,   310,   278,\n",
       "          6111,   286,  3628,  1199,   892, 14416,   491, 23772, 29879, 29889,\n",
       "           259],\n",
       "        [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     1,  6097,   895, 29901,   313,  6359, 29871,   363,\n",
       "           317,  9632,   525, 29879,  2125,   373, 11886, 29915, 29879,  1284,\n",
       "           886,  1846,    13,    13, 29950,  1478,   720,  6656, 29901,   317,\n",
       "          9632,   750,   385,  9426,   373, 11886, 29915, 29879,  1284,   886,\n",
       "         29889]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0'), 'labels': tensor([1, 0, 0, 0, 1, 0, 1, 0], device='cuda:0')}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "612ece65-37b0-4147-86da-7a57712e1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = model(input_ids=batch.input_ids, labels=batch.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c2dad3c8-7c1c-4fe4-8e12-57466f32022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7fca37b1-9095-4a7b-bb77-3b0b8d2a3e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4e8b8c66-1023-44d2-8804-dc9f5be5676c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0891421b58ea453b928b53179a442f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a44d7d33-f6a4-4610-81fc-e549785589d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "    num_rows: 24\n",
       "})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ds['train'].select(indices=range(24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a27922ca-7cf3-47e4-9476-02a144ff1ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'promptID': [31193,\n",
       "  101457,\n",
       "  134793,\n",
       "  37397,\n",
       "  50563,\n",
       "  110116,\n",
       "  42487,\n",
       "  1069,\n",
       "  23505,\n",
       "  60529,\n",
       "  55785,\n",
       "  11877,\n",
       "  32819,\n",
       "  52772,\n",
       "  102489,\n",
       "  81785,\n",
       "  16915,\n",
       "  56687,\n",
       "  101211,\n",
       "  62897,\n",
       "  38518,\n",
       "  138444,\n",
       "  76011,\n",
       "  78652],\n",
       " 'pairID': ['31193n',\n",
       "  '101457e',\n",
       "  '134793e',\n",
       "  '37397e',\n",
       "  '50563n',\n",
       "  '110116e',\n",
       "  '42487n',\n",
       "  '1069e',\n",
       "  '23505c',\n",
       "  '60529c',\n",
       "  '55785e',\n",
       "  '11877c',\n",
       "  '32819n',\n",
       "  '52772n',\n",
       "  '102489c',\n",
       "  '81785e',\n",
       "  '16915e',\n",
       "  '56687c',\n",
       "  '101211c',\n",
       "  '62897n',\n",
       "  '38518e',\n",
       "  '138444e',\n",
       "  '76011c',\n",
       "  '78652e'],\n",
       " 'premise': ['Conceptually cream skimming has two basic dimensions - product and geography.',\n",
       "  'you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him',\n",
       "  'One of our number will carry out your instructions minutely.',\n",
       "  'How do you know? All this is their information again.',\n",
       "  \"yeah i tell you what though if you go price some of those tennis shoes i can see why now you know they're getting up in the hundred dollar range\",\n",
       "  \"my walkman broke so i'm upset now i just have to turn the stereo up real loud\",\n",
       "  'But a few Christian mosaics survive above the apse is the Virgin with the infant Jesus, with the Archangel Gabriel to the right (his companion Michael, to the left, has vanished save for a few feathers from his wings).',\n",
       "  \"(Read  for Slate 's take on Jackson's findings.)\",\n",
       "  'Gays and lesbians.',\n",
       "  \"At the end of Rue des Francs-Bourgeois is what many consider to be the city's most handsome residential square, the Place des Vosges, with its stone and red brick facades.\",\n",
       "  'I burst through a set of cabin doors, and fell to the ground-',\n",
       "  'Fun for adults and children.',\n",
       "  \"It's not that the questions they asked weren't interesting or legitimate (though most did fall under the category of already asked and answered).\",\n",
       "  'Thebes held onto power until the 12th Dynasty, when its first king, Amenemhet Iwho reigned between 1980 1951 b.c. established a capital near Memphis.',\n",
       "  \"I don't mean to be glib about your concerns, but if I were you, I might be more concerned about the near-term rate implications of this $1.\",\n",
       "  'Issues in Data Synthesis.',\n",
       "  'well you see that on television also',\n",
       "  'Vrenna and I both fought him and he nearly took us.',\n",
       "  'This analysis pooled estimates from these two studies to develop a C-R function linking PM to chronic bronchitis.',\n",
       "  'He turned and smiled at Vrenna.',\n",
       "  'We sought to identify practices that were commonly implemented by the agencies within the past 5 years.',\n",
       "  'The other men shuffled.',\n",
       "  'States must show reasonable progress in their state implementation plans toward the congressionally mandated goal of returning to natural conditions in national parks and wilderness areas.',\n",
       "  \"well it's been very interesting\"],\n",
       " 'premise_binary_parse': ['( ( Conceptually ( cream skimming ) ) ( ( has ( ( ( two ( basic dimensions ) ) - ) ( ( product and ) geography ) ) ) . ) )',\n",
       "  '( you ( ( know ( during ( ( ( the season ) and ) ( i guess ) ) ) ) ( at ( at ( ( your level ) ( uh ( you ( ( ( lose them ) ( to ( the ( next level ) ) ) ) ( if ( ( if ( they ( decide ( to ( recall ( the ( the ( parent team ) ) ) ) ) ) ) ) ( ( the Braves ) ( decide ( to ( call ( to ( ( recall ( a guy ) ) ( from ( ( triple A ) ( ( ( then ( ( a ( double ( A guy ) ) ) ( ( goes up ) ( to ( replace him ) ) ) ) ) and ) ( ( a ( single ( A guy ) ) ) ( ( goes up ) ( to ( replace him ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )',\n",
       "  '( ( One ( of ( our number ) ) ) ( ( will ( ( ( carry out ) ( your instructions ) ) minutely ) ) . ) )',\n",
       "  '( ( How ( ( ( do you ) know ) ? ) ) ( ( All this ) ( ( ( is ( their information ) ) again ) . ) ) )',\n",
       "  \"( yeah ( i ( ( tell you ) ( what ( ( though ( if ( you ( go ( price ( some ( of ( those ( tennis shoes ) ) ) ) ) ) ) ) ) ( i ( can ( see ( why ( now ( you ( know ( they ( 're ( ( getting up ) ( in ( the ( hundred ( dollar range ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )\",\n",
       "  \"( ( my walkman ) ( broke ( so ( i ( 'm ( upset ( now ( i ( just ( have ( to ( ( turn ( the stereo ) ) ( up ( real loud ) ) ) ) ) ) ) ) ) ) ) ) ) )\",\n",
       "  '( But ( ( a ( few ( Christian mosaics ) ) ) ( ( survive ( above ( ( the apse ) ( ( ( is ( ( the Virgin ) ( with ( the ( infant Jesus ) ) ) ) ) , ) ( with ( ( ( the ( Archangel Gabriel ) ) ( to ( the right ) ) ) ( -LRB- ( ( ( his companion ) ( ( ( ( Michael , ) ( to ( the left ) ) ) , ) ( has ( vanished ( save ( for ( ( a ( few feathers ) ) ( from ( his wings ) ) ) ) ) ) ) ) ) -RRB- ) ) ) ) ) ) ) ) . ) ) )',\n",
       "  \"( -LRB- ( ( ( Read ( for ( ( ( Slate 's ) take ) ( on ( ( Jackson 's ) findings ) ) ) ) ) . ) -RRB- ) )\",\n",
       "  '( ( ( Gays and ) lesbians ) . )',\n",
       "  \"( ( At ( ( the end ) ( of Rue ) ) ) ( ( des Francs-Bourgeois ) ( ( is ( what ( many ( consider ( to ( ( be ( ( ( ( ( the ( city 's ) ) ( ( most handsome ) ( residential square ) ) ) , ) ( the ( Place ( des Vosges ) ) ) ) , ) ) ( with ( its ( stone ( and ( red ( brick facades ) ) ) ) ) ) ) ) ) ) ) ) . ) ) )\",\n",
       "  '( I ( ( ( ( ( burst ( through ( ( a set ) ( of ( cabin doors ) ) ) ) ) , ) and ) ( fell ( to ( the ground ) ) ) ) - ) )',\n",
       "  '( ( Fun ( for ( ( adults and ) children ) ) ) . )',\n",
       "  \"( It ( ( ( ( 's not ) ( that ( ( ( the questions ) ( they asked ) ) ( ( were n't ) ( ( interesting or ) legitimate ) ) ) ) ) ( -LRB- ( ( though ( most ( ( did fall ) ( under ( ( ( the category ) ( of already ) ) ( ( asked and ) answered ) ) ) ) ) ) -RRB- ) ) ) . ) )\",\n",
       "  '( Thebes ( ( ( ( ( held ( onto power ) ) ( until ( the ( 12th Dynasty ) ) ) ) , ) ( when ( ( ( ( its ( first king ) ) , ) ( ( Amenemhet ( Iwho reigned ) ) ( between ( 1980 ( 1951 b.c. ) ) ) ) ) ( established ( ( a capital ) ( near Memphis ) ) ) ) ) ) . ) )',\n",
       "  \"( ( ( ( ( I ( ( do n't ) ( mean ( to ( be ( glib ( about ( your concerns ) ) ) ) ) ) ) ) , ) but ) ( ( if ( I ( were you ) ) ) ( , ( I ( might ( be ( more ( concerned ( about ( ( the ( near-term ( rate implications ) ) ) ( of ( this ( $ 1 ) ) ) ) ) ) ) ) ) ) ) ) ) . )\",\n",
       "  '( ( Issues ( in ( Data Synthesis ) ) ) . )',\n",
       "  '( well ( you ( ( see ( that ( on television ) ) ) also ) ) )',\n",
       "  '( ( ( Vrenna and ) I ) ( both ( ( fought ( ( ( him and ) he ) ( nearly ( took us ) ) ) ) . ) ) )',\n",
       "  '( ( ( This analysis ) pooled ) ( ( ( estimates ( from ( these ( two studies ) ) ) ) ( to ( ( develop ( a ( C-R function ) ) ) ( ( linking PM ) ( to ( chronic bronchitis ) ) ) ) ) ) . ) )',\n",
       "  '( He ( ( ( ( turned and ) smiled ) ( at Vrenna ) ) . ) )',\n",
       "  '( We ( ( sought ( to ( identify ( practices ( that ( ( were commonly ) ( implemented ( by ( ( the agencies ) ( within ( the ( past ( 5 years ) ) ) ) ) ) ) ) ) ) ) ) ) . ) )',\n",
       "  '( ( The ( other men ) ) ( shuffled . ) )',\n",
       "  '( States ( ( must ( ( show ( ( reasonable progress ) ( in ( their ( state ( implementation plans ) ) ) ) ) ) ( toward ( ( the ( ( congressionally mandated ) goal ) ) ( of ( returning ( to ( ( natural conditions ) ( in ( national ( parks ( and ( wilderness areas ) ) ) ) ) ) ) ) ) ) ) ) ) . ) )',\n",
       "  \"( well ( it ( 's ( been ( very interesting ) ) ) ) )\"],\n",
       " 'premise_parse': ['(ROOT (S (NP (JJ Conceptually) (NN cream) (NN skimming)) (VP (VBZ has) (NP (NP (CD two) (JJ basic) (NNS dimensions)) (: -) (NP (NN product) (CC and) (NN geography)))) (. .)))',\n",
       "  '(ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN during) (NP (NP (DT the) (NN season)) (CC and) (NP (FW i) (FW guess)))) (PP (IN at) (IN at) (NP (NP (PRP$ your) (NN level)) (SBAR (S (INTJ (UH uh)) (NP (PRP you)) (VP (VBP lose) (NP (PRP them)) (PP (TO to) (NP (DT the) (JJ next) (NN level))) (SBAR (IN if) (S (SBAR (IN if) (S (NP (PRP they)) (VP (VBP decide) (S (VP (TO to) (VP (VB recall) (NP (DT the) (DT the) (NN parent) (NN team)))))))) (NP (DT the) (NNPS Braves)) (VP (VBP decide) (S (VP (TO to) (VP (VB call) (S (VP (TO to) (VP (VB recall) (NP (DT a) (NN guy)) (PP (IN from) (NP (NP (RB triple) (DT A)) (SBAR (S (S (ADVP (RB then)) (NP (DT a) (JJ double) (NNP A) (NN guy)) (VP (VBZ goes) (PRT (RP up)) (S (VP (TO to) (VP (VB replace) (NP (PRP him))))))) (CC and) (S (NP (DT a) (JJ single) (NNP A) (NN guy)) (VP (VBZ goes) (PRT (RP up)) (S (VP (TO to) (VP (VB replace) (NP (PRP him))))))))))))))))))))))))))))',\n",
       "  '(ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PRP$ our) (NN number)))) (VP (MD will) (VP (VB carry) (PRT (RP out)) (NP (PRP$ your) (NNS instructions)) (ADVP (RB minutely)))) (. .)))',\n",
       "  '(ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do) (NP (PRP you)) (VP (VB know))) (. ?)) (NP (PDT All) (DT this)) (VP (VBZ is) (NP (PRP$ their) (NN information)) (ADVP (RB again))) (. .)))',\n",
       "  \"(ROOT (S (VP (VB yeah) (S (NP (FW i)) (VP (VB tell) (NP (PRP you)) (SBAR (WHNP (WP what)) (S (SBAR (RB though) (IN if) (S (NP (PRP you)) (VP (VBP go) (VP (VB price) (NP (NP (DT some)) (PP (IN of) (NP (DT those) (NN tennis) (NNS shoes)))))))) (NP (FW i)) (VP (MD can) (VP (VB see) (SBAR (WHADVP (WRB why)) (S (ADVP (RB now)) (NP (PRP you)) (VP (VBP know) (SBAR (S (NP (PRP they)) (VP (VBP 're) (VP (VBG getting) (PRT (RP up)) (PP (IN in) (NP (DT the) (CD hundred) (NN dollar) (NN range)))))))))))))))))))\",\n",
       "  \"(ROOT (S (NP (PRP$ my) (NN walkman)) (VP (VBD broke) (SBAR (IN so) (S (NP (FW i)) (VP (VBP 'm) (ADJP (VBN upset) (SBAR (RB now) (S (NP (FW i)) (ADVP (RB just)) (VP (VBP have) (S (VP (TO to) (VP (VB turn) (NP (DT the) (NN stereo)) (ADVP (RB up) (RB real) (JJ loud)))))))))))))))\",\n",
       "  '(ROOT (S (CC But) (NP (DT a) (JJ few) (JJ Christian) (NNS mosaics)) (VP (VBP survive) (PP (IN above) (NP (NP (DT the) (NN apse)) (SBAR (S (VP (VBZ is) (NP (NP (DT the) (NN Virgin)) (PP (IN with) (NP (DT the) (JJ infant) (NN Jesus)))) (, ,) (PP (IN with) (NP (NP (DT the) (NNP Archangel) (NNP Gabriel)) (PP (TO to) (NP (DT the) (NN right))) (PRN (-LRB- -LRB-) (NP (NP (PRP$ his) (NN companion)) (SBAR (S (NP (NP (NNP Michael)) (, ,) (PP (TO to) (NP (DT the) (NN left))) (, ,)) (VP (VBZ has) (VP (VBN vanished) (S (VP (VB save) (PP (IN for) (NP (NP (DT a) (JJ few) (NNS feathers)) (PP (IN from) (NP (PRP$ his) (NNS wings)))))))))))) (-RRB- -RRB-)))))))))) (. .)))',\n",
       "  \"(ROOT (S (-LRB- -LRB-) (VP (VB Read) (PP (IN for) (NP (NP (NP (NNP Slate) (POS 's)) (NN take)) (PP (IN on) (NP (NP (NNP Jackson) (POS 's)) (NNS findings)))))) (. .) (-RRB- -RRB-)))\",\n",
       "  '(ROOT (NP (NP (NP (NNS Gays)) (CC and) (NP (NNS lesbians))) (. .)))',\n",
       "  \"(ROOT (S (PP (IN At) (NP (NP (DT the) (NN end)) (PP (IN of) (NP (NNP Rue))))) (NP (NNP des) (NNP Francs-Bourgeois)) (VP (VBZ is) (SBAR (WHNP (WP what)) (S (NP (DT many)) (VP (VBP consider) (S (VP (TO to) (VP (VB be) (NP (NP (NP (DT the) (NN city) (POS 's)) (ADJP (RBS most) (JJ handsome)) (JJ residential) (NN square)) (, ,) (NP (DT the) (NNP Place) (NNP des) (NNPS Vosges)) (, ,)) (PP (IN with) (NP (PRP$ its) (NN stone) (CC and) (JJ red) (NN brick) (NNS facades)))))))))) (. .)))\",\n",
       "  '(ROOT (S (NP (PRP I)) (VP (VP (VBP burst) (PP (IN through) (NP (NP (DT a) (NN set)) (PP (IN of) (NP (NN cabin) (NNS doors)))))) (, ,) (CC and) (VP (VBD fell) (PP (TO to) (NP (DT the) (NN ground))))) (: -)))',\n",
       "  '(ROOT (S (VP (VB Fun) (PP (IN for) (NP (NNS adults) (CC and) (NNS children)))) (. .)))',\n",
       "  \"(ROOT (S (NP (PRP It)) (VP (VBZ 's) (RB not) (SBAR (IN that) (S (NP (NP (DT the) (NNS questions)) (SBAR (S (NP (PRP they)) (VP (VBD asked))))) (VP (VBD were) (RB n't) (ADJP (JJ interesting) (CC or) (JJ legitimate))))) (PRN (-LRB- -LRB-) (SBAR (IN though) (S (NP (JJS most)) (VP (VBD did) (NP (NN fall)) (PP (IN under) (NP (NP (DT the) (NN category)) (PP (IN of) (ADVP (RB already))) (VP (VBN asked) (CC and) (VBN answered))))))) (-RRB- -RRB-))) (. .)))\",\n",
       "  '(ROOT (S (NP (NNS Thebes)) (VP (VBD held) (PP (IN onto) (NP (NN power))) (PP (IN until) (NP (DT the) (JJ 12th) (NN Dynasty))) (, ,) (SBAR (WHADVP (WRB when)) (S (NP (NP (PRP$ its) (JJ first) (NN king)) (, ,) (NP (NP (NNP Amenemhet) (NNP Iwho) (NNP reigned)) (PP (IN between) (NP (CD 1980) (CD 1951) (NNS b.c.))))) (VP (VBD established) (NP (NP (DT a) (NN capital)) (PP (IN near) (NP (NNP Memphis)))))))) (. .)))',\n",
       "  \"(ROOT (S (S (NP (PRP I)) (VP (VBP do) (RB n't) (VP (VB mean) (S (VP (TO to) (VP (VB be) (VP (VBN glib) (PP (IN about) (NP (PRP$ your) (NNS concerns)))))))))) (, ,) (CC but) (S (SBAR (IN if) (S (NP (PRP I)) (VP (VBD were) (NP (PRP you))))) (, ,) (NP (PRP I)) (VP (MD might) (VP (VB be) (ADJP (RBR more) (JJ concerned) (PP (IN about) (NP (NP (DT the) (JJ near-term) (NN rate) (NNS implications)) (PP (IN of) (NP (DT this) ($ $) (CD 1))))))))) (. .)))\",\n",
       "  '(ROOT (NP (NP (NNS Issues)) (PP (IN in) (NP (NNP Data) (NNPS Synthesis))) (. .)))',\n",
       "  '(ROOT (ADJP (RB well) (SBAR (S (NP (PRP you)) (VP (VBP see) (NP (NP (DT that)) (PP (IN on) (NP (NN television)))) (ADVP (RB also)))))))',\n",
       "  '(ROOT (S (NP (NNP Vrenna) (CC and) (PRP I)) (DT both) (VP (VBD fought) (SBAR (S (NP (PRP him) (CC and) (PRP he)) (ADVP (RB nearly)) (VP (VBD took) (NP (PRP us)))))) (. .)))',\n",
       "  '(ROOT (S (NP (NP (DT This) (NN analysis)) (VP (VBN pooled))) (VP (VBZ estimates) (PP (IN from) (NP (DT these) (CD two) (NNS studies))) (S (VP (TO to) (VP (VB develop) (NP (DT a) (JJ C-R) (NN function)) (S (VP (VBG linking) (NP (NNP PM)) (PP (TO to) (NP (JJ chronic) (NNS bronchitis))))))))) (. .)))',\n",
       "  '(ROOT (S (NP (PRP He)) (VP (VBD turned) (CC and) (VBD smiled) (PP (IN at) (NP (NNP Vrenna)))) (. .)))',\n",
       "  '(ROOT (S (NP (PRP We)) (VP (VBD sought) (S (VP (TO to) (VP (VB identify) (NP (NP (NNS practices)) (SBAR (WHNP (WDT that)) (S (VP (VBD were) (ADVP (RB commonly)) (VP (VBN implemented) (PP (IN by) (NP (NP (DT the) (NNS agencies)) (PP (IN within) (NP (DT the) (JJ past) (CD 5) (NNS years)))))))))))))) (. .)))',\n",
       "  '(ROOT (S (NP (DT The) (JJ other) (NNS men)) (VP (VBD shuffled)) (. .)))',\n",
       "  '(ROOT (S (NP (NNPS States)) (VP (MD must) (VP (VB show) (NP (NP (JJ reasonable) (NN progress)) (PP (IN in) (NP (PRP$ their) (NN state) (NN implementation) (NNS plans)))) (PP (IN toward) (NP (NP (DT the) (ADJP (RB congressionally) (VBN mandated)) (NN goal)) (PP (IN of) (S (VP (VBG returning) (PP (TO to) (NP (NP (JJ natural) (NNS conditions)) (PP (IN in) (NP (JJ national) (NNS parks) (CC and) (NN wilderness) (NNS areas)))))))))))) (. .)))',\n",
       "  \"(ROOT (ADJP (RB well) (SBAR (S (NP (PRP it)) (VP (VBZ 's) (VP (VBN been) (ADJP (RB very) (JJ interesting))))))))\"],\n",
       " 'hypothesis': ['Product and geography are what make cream skimming work. ',\n",
       "  'You lose the things to the following level if the people recall.',\n",
       "  'A member of my team will execute your orders with immense precision.',\n",
       "  'This information belongs to them.',\n",
       "  'The tennis shoes have a range of prices.',\n",
       "  \"I'm upset that my walkman broke and now I have to turn the stereo up really loud.\",\n",
       "  'Most of the Christian mosaics were destroyed by Muslims.  ',\n",
       "  \"Slate had an opinion on Jackson's findings.\",\n",
       "  'Heterosexuals.',\n",
       "  'Place des Vosges is constructed entirely of gray marble.',\n",
       "  'I burst through the doors and fell down.',\n",
       "  'Fun for only children.',\n",
       "  'All of the questions were interesting according to a focus group consulted on the subject.',\n",
       "  'The capital near Memphis lasted only half a century before its inhabitants abandoned it for the next capital. ',\n",
       "  'I am concerned more about your issues than the near-term rate implications.',\n",
       "  'Problems in data synthesis.',\n",
       "  'You can see that on television, as well.',\n",
       "  'Neither Vrenna nor myself have ever fought him.',\n",
       "  'The analysis proves that there is no link between PM and bronchitis.',\n",
       "  'He smiled at Vrenna who was walking slowly behind him with her mother.',\n",
       "  'We want to identify practices commonly used by agencies in the last 5 years',\n",
       "  'The other men were shuffled around.',\n",
       "  'Itis not necessary for there to be any improvement.',\n",
       "  'It has been very intriguing.'],\n",
       " 'hypothesis_binary_parse': ['( ( ( Product and ) geography ) ( ( are ( what ( make ( cream ( skimming work ) ) ) ) ) . ) )',\n",
       "  '( You ( ( ( ( lose ( the things ) ) ( to ( the ( following level ) ) ) ) ( if ( ( the people ) recall ) ) ) . ) )',\n",
       "  '( ( ( A member ) ( of ( my team ) ) ) ( ( will ( ( execute ( your orders ) ) ( with ( immense precision ) ) ) ) . ) )',\n",
       "  '( ( This information ) ( ( belongs ( to them ) ) . ) )',\n",
       "  '( ( The ( tennis shoes ) ) ( ( have ( ( a range ) ( of prices ) ) ) . ) )',\n",
       "  \"( ( ( ( I ( 'm ( upset ( that ( ( my walkman ) broke ) ) ) ) ) and ) ( now ( I ( have ( to ( ( turn ( the stereo ) ) ( up ( really loud ) ) ) ) ) ) ) ) . )\",\n",
       "  '( ( Most ( of ( the ( Christian mosaics ) ) ) ) ( ( were ( destroyed ( by Muslims ) ) ) . ) )',\n",
       "  \"( Slate ( ( had ( ( an opinion ) ( on ( ( Jackson 's ) findings ) ) ) ) . ) )\",\n",
       "  '( Heterosexuals . )',\n",
       "  '( ( Place ( des Vosges ) ) ( ( is ( ( constructed entirely ) ( of ( gray marble ) ) ) ) . ) )',\n",
       "  '( I ( ( ( ( burst ( through ( the doors ) ) ) and ) ( fell down ) ) . ) )',\n",
       "  '( ( Fun ( for ( only children ) ) ) . )',\n",
       "  '( ( All ( of ( the questions ) ) ) ( ( ( were interesting ) ( according ( to ( ( a ( focus group ) ) ( consulted ( on ( the subject ) ) ) ) ) ) ) . ) )',\n",
       "  '( ( ( The capital ) ( near Memphis ) ) ( ( ( ( lasted ( only half ) ) ( a century ) ) ( before ( ( its inhabitants ) ( ( abandoned it ) ( for ( the ( next capital ) ) ) ) ) ) ) . ) )',\n",
       "  '( I ( ( am ( ( concerned ( more ( about ( your issues ) ) ) ) ( than ( the ( near-term ( rate implications ) ) ) ) ) ) . ) )',\n",
       "  '( ( Problems ( in ( data synthesis ) ) ) . )',\n",
       "  '( You ( ( can ( ( ( see ( that ( on television ) ) ) , ) ( as well ) ) ) . ) )',\n",
       "  '( ( Neither ( ( Vrenna nor ) myself ) ) ( ( ( have ever ) ( fought him ) ) . ) )',\n",
       "  '( ( The analysis ) ( ( proves ( that ( there ( ( ( is ( ( no link ) ( between PM ) ) ) and ) bronchitis ) ) ) ) . ) )',\n",
       "  '( He ( ( smiled ( at ( Vrenna ( who ( was ( ( walking ( slowly ( behind him ) ) ) ( with ( her mother ) ) ) ) ) ) ) ) . ) )',\n",
       "  '( We ( want ( to ( identify ( practices ( commonly ( used ( by ( agencies ( in ( the ( last ( 5 years ) ) ) ) ) ) ) ) ) ) ) ) )',\n",
       "  '( ( The ( other men ) ) ( ( were ( shuffled around ) ) . ) )',\n",
       "  '( ( ( Itis ( not necessary ) ) ( for ( there ( to ( be ( any improvement ) ) ) ) ) ) . )',\n",
       "  '( It ( ( has ( been ( very intriguing ) ) ) . ) )'],\n",
       " 'hypothesis_parse': ['(ROOT (S (NP (NN Product) (CC and) (NN geography)) (VP (VBP are) (SBAR (WHNP (WP what)) (S (VP (VBP make) (NP (NP (NN cream)) (VP (VBG skimming) (NP (NN work)))))))) (. .)))',\n",
       "  '(ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT the) (NNS things)) (PP (TO to) (NP (DT the) (JJ following) (NN level))) (SBAR (IN if) (S (NP (DT the) (NNS people)) (VP (VBP recall))))) (. .)))',\n",
       "  '(ROOT (S (NP (NP (DT A) (NN member)) (PP (IN of) (NP (PRP$ my) (NN team)))) (VP (MD will) (VP (VB execute) (NP (PRP$ your) (NNS orders)) (PP (IN with) (NP (JJ immense) (NN precision))))) (. .)))',\n",
       "  '(ROOT (S (NP (DT This) (NN information)) (VP (VBZ belongs) (PP (TO to) (NP (PRP them)))) (. .)))',\n",
       "  '(ROOT (S (NP (DT The) (NN tennis) (NNS shoes)) (VP (VBP have) (NP (NP (DT a) (NN range)) (PP (IN of) (NP (NNS prices))))) (. .)))',\n",
       "  \"(ROOT (S (S (NP (PRP I)) (VP (VBP 'm) (ADJP (VBN upset) (SBAR (IN that) (S (NP (PRP$ my) (NN walkman)) (VP (VBD broke))))))) (CC and) (S (ADVP (RB now)) (NP (PRP I)) (VP (VBP have) (S (VP (TO to) (VP (VB turn) (NP (DT the) (NN stereo)) (ADVP (RB up) (RB really) (JJ loud))))))) (. .)))\",\n",
       "  '(ROOT (S (NP (NP (JJS Most)) (PP (IN of) (NP (DT the) (JJ Christian) (NNS mosaics)))) (VP (VBD were) (VP (VBN destroyed) (PP (IN by) (NP (NNPS Muslims))))) (. .)))',\n",
       "  \"(ROOT (S (NP (NNP Slate)) (VP (VBD had) (NP (NP (DT an) (NN opinion)) (PP (IN on) (NP (NP (NNP Jackson) (POS 's)) (NNS findings))))) (. .)))\",\n",
       "  '(ROOT (NP (NNS Heterosexuals) (. .)))',\n",
       "  '(ROOT (S (NP (NNP Place) (FW des) (NNP Vosges)) (VP (VBZ is) (VP (VBN constructed) (ADVP (RB entirely)) (PP (IN of) (NP (JJ gray) (NN marble))))) (. .)))',\n",
       "  '(ROOT (S (NP (PRP I)) (VP (VP (VBP burst) (PP (IN through) (NP (DT the) (NNS doors)))) (CC and) (VP (VBD fell) (PRT (RP down)))) (. .)))',\n",
       "  '(ROOT (S (VP (VB Fun) (PP (IN for) (NP (JJ only) (NNS children)))) (. .)))',\n",
       "  '(ROOT (S (NP (NP (DT All)) (PP (IN of) (NP (DT the) (NNS questions)))) (VP (VBD were) (ADJP (JJ interesting)) (PP (VBG according) (PP (TO to) (NP (NP (DT a) (NN focus) (NN group)) (VP (VBN consulted) (PP (IN on) (NP (DT the) (NN subject)))))))) (. .)))',\n",
       "  '(ROOT (S (NP (NP (DT The) (NN capital)) (PP (IN near) (NP (NNP Memphis)))) (VP (VBD lasted) (NP (QP (RB only) (PDT half))) (NP (DT a) (NN century)) (SBAR (IN before) (S (NP (PRP$ its) (NN inhabitants)) (VP (VBD abandoned) (NP (PRP it)) (PP (IN for) (NP (DT the) (JJ next) (NN capital))))))) (. .)))',\n",
       "  '(ROOT (S (NP (PRP I)) (VP (VBP am) (VP (VBN concerned) (ADVP (RBR more) (PP (IN about) (NP (PRP$ your) (NNS issues)))) (PP (IN than) (NP (DT the) (JJ near-term) (NN rate) (NNS implications))))) (. .)))',\n",
       "  '(ROOT (NP (NP (NNS Problems)) (PP (IN in) (NP (NN data) (NN synthesis))) (. .)))',\n",
       "  '(ROOT (S (NP (PRP You)) (VP (MD can) (VP (VB see) (NP (NP (DT that)) (PP (IN on) (NP (NN television)))) (, ,) (ADVP (RB as) (RB well)))) (. .)))',\n",
       "  '(ROOT (S (NP (CC Neither) (NP (NNP Vrenna)) (CC nor) (NP (PRP myself))) (VP (VBP have) (ADVP (RB ever)) (VP (VBN fought) (NP (PRP him)))) (. .)))',\n",
       "  '(ROOT (S (NP (DT The) (NN analysis)) (VP (VBZ proves) (SBAR (IN that) (S (NP (EX there)) (VP (VP (VBZ is) (NP (NP (DT no) (NN link)) (PP (IN between) (NP (NNP PM))))) (CC and) (VP (VBZ bronchitis)))))) (. .)))',\n",
       "  '(ROOT (S (NP (PRP He)) (VP (VBD smiled) (PP (IN at) (NP (NP (NNP Vrenna)) (SBAR (WHNP (WP who)) (S (VP (VBD was) (VP (VBG walking) (ADVP (RB slowly) (PP (IN behind) (NP (PRP him)))) (PP (IN with) (NP (PRP$ her) (NN mother)))))))))) (. .)))',\n",
       "  '(ROOT (S (NP (PRP We)) (VP (VBP want) (S (VP (TO to) (VP (VB identify) (NP (NP (NNS practices)) (VP (ADVP (RB commonly)) (VBN used) (PP (IN by) (NP (NP (NNS agencies)) (PP (IN in) (NP (DT the) (JJ last) (CD 5) (NNS years)))))))))))))',\n",
       "  '(ROOT (S (NP (DT The) (JJ other) (NNS men)) (VP (VBD were) (VP (VBN shuffled) (PRT (RP around)))) (. .)))',\n",
       "  '(ROOT (S (VP (VB Itis) (ADJP (RB not) (JJ necessary)) (SBAR (IN for) (S (NP (EX there)) (VP (TO to) (VP (VB be) (NP (DT any) (NN improvement))))))) (. .)))',\n",
       "  '(ROOT (S (NP (PRP It)) (VP (VBZ has) (VP (VBN been) (ADJP (RB very) (JJ intriguing)))) (. .)))'],\n",
       " 'genre': ['government',\n",
       "  'telephone',\n",
       "  'fiction',\n",
       "  'fiction',\n",
       "  'telephone',\n",
       "  'telephone',\n",
       "  'travel',\n",
       "  'slate',\n",
       "  'slate',\n",
       "  'travel',\n",
       "  'fiction',\n",
       "  'travel',\n",
       "  'slate',\n",
       "  'travel',\n",
       "  'government',\n",
       "  'government',\n",
       "  'telephone',\n",
       "  'fiction',\n",
       "  'government',\n",
       "  'fiction',\n",
       "  'government',\n",
       "  'fiction',\n",
       "  'government',\n",
       "  'telephone'],\n",
       " 'label': [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0]}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ds['train'][:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8073e14-2150-4b70-bb83-32a0e3c5376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.compute(predictions=predicts, references=actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa4f1b-5ddb-4cfd-b235-f3d8f6301466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics.acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e78c0ba1-b7ce-4cde-b1e7-5ce8dea372d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de995c33eaa44fa88c7577a8286d789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"nyu-mll/multi_nli\")\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama_v1.1\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama_v1.1\")\n",
    "if tokenizer.pad_token == None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "def preprocess_function(examples, tokenizer):\n",
    "    new_examples = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": [],\n",
    "    }\n",
    "    for premise, hypothesis, labels in zip(examples[\"premise\"], examples[\"hypothesis\"], examples['label']):\n",
    "        tokenized = tokenizer(\"Premise: \" + premise + \"\\n\\nHypothesis: \" + hypothesis)\n",
    "        new_examples[\"input_ids\"].append(tokenized[\"input_ids\"])\n",
    "        new_examples[\"attention_mask\"].append(tokenized[\"attention_mask\"])\n",
    "        new_examples['labels'].append(labels)\n",
    "    return new_examples\n",
    "\n",
    "\n",
    "import os\n",
    "num_proc = 1 #int(os.cpu_count()*0.95)\n",
    "# num_proc = 2\n",
    "\n",
    "column_names = ds['train'].column_names\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda example : preprocess_function(example, tokenizer),\n",
    "    batched=True,\n",
    "    num_proc=num_proc,\n",
    "    remove_columns=column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9f44ba29-9364-4435-be35-1c02a1023c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c2235705-6c24-4231-9c52-e39fe6f50210",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fda1e71e-3f7f-4eb7-bfed-56570eea77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c89c0611-d12a-4526-a81f-e3b6067a857f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbac6e2-31cb-48b5-b36a-0d968c83bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoModelForSequenceClassification.from_pretrained("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1a348dd-c839-488a-8dbd-17d36067ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"TinyLlama/TinyLlama_v1.1\"a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7de741fe-6266-431a-833e-6901ab117aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c4cbf641-af6e-434b-98f1-06c4c1dec834",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\projects\\transformers\\src\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32m~\\projects\\transformers\\src\\transformers\\modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\projects\\transformers\\src\\transformers\\modeling_utils.py:4224\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4222\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[0;32m   4223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_attn_implementation_autoset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 4224\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autoset_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[0;32m   4226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4228\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m   4229\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m   4230\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n",
      "File \u001b[1;32m~\\projects\\transformers\\src\\transformers\\modeling_utils.py:1615\u001b[0m, in \u001b[0;36mPreTrainedModel._autoset_attn_implementation\u001b[1;34m(cls, config, use_flash_attention_2, torch_dtype, device_map, check_device_map)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1615\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_and_enable_flash_attn_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhard_check_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1621\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m requested_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflex_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1623\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_enable_flex_attn(config, hard_check_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\projects\\transformers\\src\\transformers\\modeling_utils.py:1750\u001b[0m, in \u001b[0;36mPreTrainedModel._check_and_enable_flash_attn_2\u001b[1;34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001b[0m\n\u001b[0;32m   1747\u001b[0m install_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m the package flash_attn seems to be not installed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1752\u001b[0m flash_attention_version \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda:\n",
      "\u001b[1;31mImportError\u001b[0m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(path, attn_implementation=\"flash_attention_2\", trust_remote_code=True, torch_dtype=torch.bfloat16, %notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42e36b-7d5a-4750-90a5-aa6b60ecfc21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a921128e-51c7-4047-ac06-7ffc3a01e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c5f6d972-48f6-4a60-9cf0-d8e4c71ac6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2805b3e5-a106-4065-8011-c43c7b651618",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3172cae2-b552-4d5f-bbaa-553aad87b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token == None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c9d603a8-a11c-45bc-bb14-ddce11e97e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "86028507-7ee3-441c-a74e-e44ff5057ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b7e81d-f241-435e-ada2-892cd5de8f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Conceptually cream skimming has two basic dimensions - product and geography.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]['premise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35b4e854-32ac-4527-bb8d-f6b67050acac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Product and geography are what make cream skimming work. '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]['hypothesis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ce583-3a26-42b6-a3d7-a40ecd1297b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "entailment (0), neutral (1), contradiction (2)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
